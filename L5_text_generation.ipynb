{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 5: Text Generation with Vertex AI"
      ],
      "metadata": {
        "id": "vMirJLJtemc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Project environment setup\n",
        "\n",
        "- Load credentials and relevant Python Libraries"
      ],
      "metadata": {
        "id": "7KAsub0eeoWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import authenticate\n",
        "credentials, PROJECT_ID = authenticate()"
      ],
      "metadata": {
        "id": "gqzWpwgjenu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = 'us-central1'"
      ],
      "metadata": {
        "id": "RHDkNGF6etOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt the model\n",
        "- We'll import a language model that has been trained to handle a variety of natural language tasks, `text-bison@001`.\n",
        "- For multi-turn dialogue with a language model, you can use, `chat-bison@001`."
      ],
      "metadata": {
        "id": "CVo1j9GSevi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID,\n",
        "              location=REGION,\n",
        "              credentials = credentials)"
      ],
      "metadata": {
        "id": "mzjfkexHeyo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextGenerationModel"
      ],
      "metadata": {
        "id": "yGUFcA8Be1PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_model = TextGenerationModel.from_pretrained(\n",
        "    \"text-bison@001\")"
      ],
      "metadata": {
        "id": "dqWWXdPTe1Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question Answering\n",
        "- You can ask an open-ended question to the language model."
      ],
      "metadata": {
        "id": "j6BimOoOe9dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I'm a high school student. \\\n",
        "Recommend me a programming activity to improve my skills.\""
      ],
      "metadata": {
        "id": "IIuRmrJXe-Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generation_model.predict(prompt=prompt).text)"
      ],
      "metadata": {
        "id": "qgnkOMhsfAWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classify and elaborate\n",
        "- For more predictability of the language model's response, you can also ask the language model to choose among a list of answers and then elaborate on its answer."
      ],
      "metadata": {
        "id": "H9NC7h1EfEeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a high school student. \\\n",
        "Which of these activities do you suggest and why:\n",
        "a) learn Python\n",
        "b) learn Javascript\n",
        "c) learn Fortran\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "H1vUkeNhfFVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generation_model.predict(prompt=prompt).text)"
      ],
      "metadata": {
        "id": "bguYTeh2fH40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract information and format it as a table"
      ],
      "metadata": {
        "id": "M5WptKf9fNa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\" A bright and promising wildlife biologist \\\n",
        "named Jesse Plank (Amara Patel) is determined to make her \\\n",
        "mark on the world.\n",
        "Jesse moves to Texas for what she believes is her dream job,\n",
        "only to discover a dark secret that will make \\\n",
        "her question everything.\n",
        "In the new lab she quickly befriends the outgoing \\\n",
        "lab tech named Maya Jones (Chloe Nguyen),\n",
        "and the lab director Sam Porter (Fredrik Johansson).\n",
        "Together the trio work long hours on their research \\\n",
        "in a hope to change the world for good.\n",
        "Along the way they meet the comical \\\n",
        "Brenna Ode (Eleanor Garcia) who is a marketing lead \\\n",
        "at the research institute,\n",
        "and marine biologist Siri Teller (Freya Johansson).\n",
        "\n",
        "Extract the characters, their jobs \\\n",
        "and the actors who played them from the above message as a table\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sOr_llZ_fOLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(prompt=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "jVmjBMjYfRI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adjusting Creativity/Randomness\n",
        "- You can control the behavior of the language model's decoding strategy by adjusting the temperature, top-k, and top-n parameters.\n",
        "- For tasks for which you want the model to consistently output the same result for the same input, (such as classification or information extraction), set temperature to zero.\n",
        "- For tasks where you desire more creativity, such as brainstorming, summarization, choose a higher temperature (up to 1)."
      ],
      "metadata": {
        "id": "E6_QGnIDfWDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.0"
      ],
      "metadata": {
        "id": "mfwLo2CDfWl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Complete the sentence: \\\n",
        "As I prepared the picture frame, \\\n",
        "I reached into my toolkit to fetch my:\""
      ],
      "metadata": {
        "id": "MDn_hOSYfe7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(\n",
        "    prompt=prompt,\n",
        "    temperature=temperature,\n",
        ")"
      ],
      "metadata": {
        "id": "gYHeHwEQffHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[temperature = {temperature}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "_jMOWup4ffSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 1.0"
      ],
      "metadata": {
        "id": "WZSCfzqQffcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(\n",
        "    prompt=prompt,\n",
        "    temperature=temperature,\n",
        ")"
      ],
      "metadata": {
        "id": "o_wQp0pQffnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[temperature = {temperature}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "u6Zklttxffw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top P\n",
        "- Top p: sample the minimum set of tokens whose probabilities add up to probability `p` or greater.\n",
        "- The default value for `top_p` is `0.95`.\n",
        "- If you want to adjust `top_p` and `top_k` and see different results, remember to set `temperature` to be greater than zero, otherwise the model will always choose the token with the highest probability."
      ],
      "metadata": {
        "id": "Bk6sZcyjf6dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p = 0.2"
      ],
      "metadata": {
        "id": "sRQBrosTf-Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an advertisement for jackets \\\n",
        "that involves blue elephants and avocados.\""
      ],
      "metadata": {
        "id": "_yZXX0mxgBET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(\n",
        "    prompt=prompt,\n",
        "    temperature=0.9,\n",
        "    top_p=top_p,\n",
        ")"
      ],
      "metadata": {
        "id": "SOVlRs3RgBOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[top_p = {top_p}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "VfPJWP1_gBcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Top k\n",
        "- The default value for `top_k` is `40`.\n",
        "- You can set `top_k` to values between `1` and `40`.\n",
        "- The decoding strategy applies `top_k`, then `top_p`, then `temperature` (in that order)."
      ],
      "metadata": {
        "id": "dCe6ebAzgLEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 20\n",
        "top_p = 0.7"
      ],
      "metadata": {
        "id": "23WHukxZgL98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generation_model.predict(\n",
        "    prompt=prompt,\n",
        "    temperature=0.9,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p,\n",
        ")"
      ],
      "metadata": {
        "id": "l9-RKZZfgNr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[top_p = {top_p}]\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "R-sv2PRIgN-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}